{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b007ad4-01a6-48f4-8193-d9637954cad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45925a-1ea8-4750-8ab7-0db0bcc904a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### API Calling to Collect Required Data : \n",
    "This script collects financial data for MAMAA companies (Meta, Apple, Microsoft, Amazon and Alphabet) companies using the Alpha Vantage API. It fetches weekly and monthly adjusted time series data, company overview data, and technical indicators such as SMA, EMA, and RSI, and stores this data in JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec5d67-078a-41ad-af79-29a5609edee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbe092-a071-429f-83f8-8c86402a325d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "api_key = 'API-KEY 1'\n",
    "\n",
    "#MAMAA Companies\n",
    "symbols = ['META', 'AAPL', 'MSFT', 'AMZN', 'GOOGL']\n",
    "\n",
    "all_daily_data = []\n",
    "all_weekly_data = []\n",
    "all_monthly_data = []\n",
    "all_overview_data = []\n",
    "\n",
    "\n",
    "# Function to fetch data from a given API endpoint\n",
    "def fetch_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        data = response.json()\n",
    "        if \"Error Message\" in data or \"Note\" in data:  # Check for API error messages\n",
    "            print(f\"Error fetching data: {data.get('Error Message', data.get('Note'))}\")\n",
    "            return None\n",
    "        time.sleep(12)  # Sleep to respect API rate limits\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through each symbol and fetch the required data\n",
    "for symbol in symbols:\n",
    "    # Fetch weekly adjusted data\n",
    "    daily_url = f'https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY_ADJUSTED&symbol={symbol}&apikey={api_key}'\n",
    "    daily_data = fetch_data(daily_url)\n",
    "    if daily_data:\n",
    "        all_daily_data.append(daily_data)\n",
    "    \n",
    "    # Fetch company overview data\n",
    "    overview_url = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={api_key}'\n",
    "    overview_data = fetch_data(overview_url)\n",
    "    if overview_data:\n",
    "        all_overview_data.append(overview_data)\n",
    "\n",
    "\n",
    "# Save the fetched data to JSON files\n",
    "with open('weekly_data.json', 'w') as f:\n",
    "    json.dump(all_weekly_data, f)\n",
    "with open('monthly_data.json', 'w') as f:\n",
    "    json.dump(all_monthly_data, f)\n",
    "with open('overview_data.json', 'w') as f:\n",
    "    json.dump(all_overview_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a185c68-a4c2-4aba-a3eb-60794c279e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "api_key = 'API-KEY 2'\n",
    "\n",
    "#MAMAA Companies\n",
    "symbols = ['META', 'AAPL', 'MSFT', 'AMZN', 'GOOGL']\n",
    "\n",
    "all_sma_data = []\n",
    "all_ema_data = []\n",
    "all_rsi_data = []\n",
    "\n",
    "# Loop through each symbol and fetch the required data\n",
    "for symbol in symbols:\n",
    "    \n",
    "    # Fetch SMA data\n",
    "    sma_url = f'https://www.alphavantage.co/query?function=SMA&symbol={symbol}&interval=weekly&time_period=10&series_type=close&apikey={api_key}'\n",
    "    sma_data = fetch_data(sma_url)\n",
    "    if sma_data:\n",
    "        all_sma_data.append(sma_data)\n",
    "    \n",
    "    # Fetch EMA data\n",
    "    ema_url = f'https://www.alphavantage.co/query?function=EMA&symbol={symbol}&interval=weekly&time_period=10&series_type=close&apikey={api_key}'\n",
    "    ema_data = fetch_data(ema_url)\n",
    "    if ema_data:\n",
    "        all_ema_data.append(ema_data)\n",
    "\n",
    "    # Fetch RSI data\n",
    "    rsi_url = f'https://www.alphavantage.co/query?function=RSI&symbol={symbol}&interval=weekly&time_period=14&series_type=close&apikey={api_key}'\n",
    "    rsi_data = fetch_data(rsi_url)\n",
    "    if rsi_data:\n",
    "        all_rsi_data.append(rsi_data)\n",
    "        \n",
    "with open('sma_data.json', 'w') as f:\n",
    "    json.dump(all_sma_data, f)\n",
    "with open('ema_data.json', 'w') as f:\n",
    "    json.dump(all_ema_data, f)\n",
    "with open('rsi_data.json', 'w') as f:\n",
    "    json.dump(all_rsi_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e911cef-52c9-4d07-9596-5255b535d2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "We had to use 2 API keys because Alpha Vantage standard API rate limit is 25 requests per day and for all the required data we needed 30 API request to collect all the required data . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844abd2-2588-4339-a6ab-3591e656bc03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54931da2-2c36-4467-a65f-12043b45eb40",
   "metadata": {},
   "source": [
    "### Structuring, Cleaning and Preparing Data : \n",
    "This script transforms the collected financial data for MAMAA companies (Meta, Apple, Microsoft, Amazon, and Alphabet). It loads the raw data from JSON files, converts it into structured DataFrames, cleans and formats the data, renames the columns for consistency, and prepares the data for loading into a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9e267-d11d-4b7a-972e-db75ca8ca6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269027d-18c7-4d6e-8618-56bb05b2327e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "def json_to_dataframe(data, key, symbol_key, indicator_type=None):\n",
    "    frames = []\n",
    "    for entry in data:\n",
    "        try:\n",
    "            meta_data = entry['Meta Data']\n",
    "            symbol = meta_data[symbol_key]\n",
    "            time_series = entry[key]\n",
    "            \n",
    "            df = pd.DataFrame(time_series).transpose().reset_index()\n",
    "            df = df.rename(columns={'index': 'date'})\n",
    "            \n",
    "            df['symbol'] = symbol\n",
    "            frames.append(df)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} in entry: {entry}\")\n",
    "            continue\n",
    "        \n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Load JSON data\n",
    "daily_data = load_json('daily_data.json')\n",
    "sma_data = load_json('sma_data.json')\n",
    "ema_data = load_json('ema_data.json')\n",
    "rsi_data = load_json('rsi_data.json')\n",
    "overview_data = load_json('overview_data.json')\n",
    "\n",
    "\n",
    "# Convert overview JSON data to DataFrame\n",
    "def json_to_dataframe_overview(data):\n",
    "    # Define the schema fields\n",
    "    fields = [\n",
    "        'Symbol', 'AssetType', 'Name', 'Description', 'CIK', 'Exchange', 'Currency', 'Country', \n",
    "        'Sector', 'Industry', 'Address', 'FiscalYearEnd', 'LatestQuarter', 'MarketCapitalization', \n",
    "        'EBITDA', 'PERatio', 'PEGRatio', 'BookValue', 'DividendPerShare', 'DividendYield', 'EPS', \n",
    "        'RevenuePerShareTTM', 'ProfitMargin', 'OperatingMarginTTM', 'ReturnOnAssetsTTM', \n",
    "        'ReturnOnEquityTTM', 'RevenueTTM', 'GrossProfitTTM', 'DilutedEPSTTM', \n",
    "        'QuarterlyEarningsGrowthYOY', 'QuarterlyRevenueGrowthYOY', 'AnalystTargetPrice', \n",
    "        'AnalystRatingStrongBuy', 'AnalystRatingBuy', 'AnalystRatingHold', 'AnalystRatingSell', \n",
    "        'AnalystRatingStrongSell', 'TrailingPE', 'ForwardPE', 'PriceToSalesRatioTTM', \n",
    "        'PriceToBookRatio', 'EVToRevenue', 'EVToEBITDA', 'Beta', '52WeekHigh', '52WeekLow', \n",
    "        '50DayMovingAverage', '200DayMovingAverage', 'SharesOutstanding'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame from JSON data\n",
    "    df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "    # Convert date fields to datetime\n",
    "    df['LatestQuarter'] = pd.to_datetime(df['LatestQuarter'])\n",
    "\n",
    "    # Convert numeric fields to appropriate types\n",
    "    numeric_fields = [\n",
    "        'MarketCapitalization', 'EBITDA', 'PERatio', 'PEGRatio', 'BookValue', 'DividendPerShare', \n",
    "        'DividendYield', 'EPS', 'RevenuePerShareTTM', 'ProfitMargin', 'OperatingMarginTTM', 'ReturnOnAssetsTTM', \n",
    "        'ReturnOnEquityTTM', 'RevenueTTM', 'GrossProfitTTM', 'DilutedEPSTTM', \n",
    "        'QuarterlyEarningsGrowthYOY', 'QuarterlyRevenueGrowthYOY', 'AnalystTargetPrice', \n",
    "        'AnalystRatingStrongBuy', 'AnalystRatingBuy', 'AnalystRatingHold', 'AnalystRatingSell', \n",
    "        'AnalystRatingStrongSell', 'TrailingPE', 'ForwardPE', 'PriceToSalesRatioTTM', \n",
    "        'PriceToBookRatio', 'EVToRevenue', 'EVToEBITDA', 'Beta', '52WeekHigh', '52WeekLow', \n",
    "        '50DayMovingAverage', '200DayMovingAverage', 'SharesOutstanding',\n",
    "    ]\n",
    "\n",
    "    for field in numeric_fields:\n",
    "        df[field] = pd.to_numeric(df[field], errors='coerce')\n",
    "        \n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Convert JSON data to DataFrames\n",
    "daily_df = json_to_dataframe(all_daily_data, 'Time Series (Daily)', '2. Symbol')\n",
    "sma_df = json_to_dataframe(sma_data, 'Technical Analysis: SMA', '1: Symbol', 'SMA')\n",
    "ema_df = json_to_dataframe(ema_data, 'Technical Analysis: EMA', '1: Symbol', 'EMA')\n",
    "rsi_df = json_to_dataframe(rsi_data, 'Technical Analysis: RSI', '1: Symbol', 'RSI')\n",
    "\n",
    "overview_df = json_to_dataframe_overview(overview_data)\n",
    "\n",
    "# Function to clean and transform the data\n",
    "def clean_transform(df, date_col='date'):\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Convert numerical columns to appropriate types\n",
    "    for col in df.columns:\n",
    "        if col not in [date_col, 'symbol']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning and transformation\n",
    "daily_df = clean_transform(daily_df)\n",
    "sma_df = clean_transform(sma_df)\n",
    "ema_df = clean_transform(ema_df)\n",
    "rsi_df = clean_transform(rsi_df)\n",
    "\n",
    "# Combine technical indicators into a single DataFrame\n",
    "tech_indicators_df = pd.concat([sma_df, ema_df, rsi_df], ignore_index=True)\n",
    "\n",
    "# Save DataFrames to CSV files (or directly to the database)\n",
    "daily_df.to_csv('daily_data.csv', index=False)\n",
    "tech_indicators_df.to_csv('tech_indicators.csv', index=False)\n",
    "overview_df.to_csv('overview_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987cb4f2-374b-4614-93d6-f0f882bc2200",
   "metadata": {},
   "source": [
    "Saving the transformed data in CSV files is important for data integrity, reusability, sharing, debugging, validation, version control, and preparing for database loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf1511-3386-4979-9f59-dd03f5d3b67a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9c37d-db7e-49bc-985b-92f5521081b9",
   "metadata": {},
   "source": [
    "### Data into the Database :\n",
    "This script loads the transformed financial data for MAMAA companies (Meta, Apple, Microsoft, Amazon, and Alphabet) from CSV files into a MySQL database. It connects to the database, creates the necessary tables if they don't exist, and inserts the data into these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fea21b-feb6-4390-9f5f-4b2fd62c6927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6878a8-3106-4eb5-ad46-191b6f738cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Up MySQL Connection\n",
    "def create_connection():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='HOST',\n",
    "            user='USER',\n",
    "            password='PASSWORD',\n",
    "            database='DATABSE NAME',\n",
    "            auth_plugin='mysql_native_password'\n",
    "        )\n",
    "        if connection.is_connected():\n",
    "            print(\"Connection to MySQL database successful\")\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute(\"SHOW TABLES;\")\n",
    "            for table in cursor.fetchall():\n",
    "                print(table)\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error: '{e}'\")\n",
    "        return None\n",
    "\n",
    "connection = create_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e5e94-8c8b-4b23-93cf-9fb3e7367c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the connection string for SQLAlchemy\n",
    "db_connection_str = 'mysql+pymysql://root:Theteam27.@localhost/mamaa'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# Function to save DataFrame to MySQL\n",
    "def save_to_mysql(df, table_name):\n",
    "    try:\n",
    "        df.to_sql(name=table_name, con=db_connection, if_exists='append', index=False)\n",
    "        print(f\"Data loaded into {table_name} successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: '{e}'\")\n",
    "\n",
    "# Load data into MySQL\n",
    "save_to_mysql(overview_df, 'companies')\n",
    "save_to_mysql(daily_df, 'daily_data')\n",
    "save_to_mysql(tech_indicators_df, 'technical_indicators')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af39d8-f0c0-40a6-8cd0-554ac0e49bb6",
   "metadata": {},
   "source": [
    "Instead of writing query to insert into database, data was directly inserted to database using - <br>\n",
    "DataFrame.to_sql(name, con, *, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)\n",
    "<br>\n",
    "To write records stored in a DataFrame to a SQL database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
